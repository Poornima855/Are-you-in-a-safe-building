{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\iamso\\anaconda3\\envs\\mlgpu\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\iamso\\anaconda3\\envs\\mlgpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\iamso\\anaconda3\\envs\\mlgpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "c:\\Users\\iamso\\anaconda3\\envs\\mlgpu\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=EfficientNet_B0_Weights.IMAGENET1K_V1`. You can also use `weights=EfficientNet_B0_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "C:\\Users\\iamso\\AppData\\Local\\Temp\\ipykernel_24152\\3011050421.py:109: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()  # For mixed precision training\n",
      "C:\\Users\\iamso\\AppData\\Local\\Temp\\ipykernel_24152\\3011050421.py:133: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():  # Mixed precision\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training VGG16...\n",
      "Epoch 1/50, Training Loss: 0.4014, Training Accuracy: 29.87%, Validation Loss: 1.5616, Validation Accuracy: 44.83%\n",
      "Epoch 2/50, Training Loss: 0.3747, Training Accuracy: 34.74%, Validation Loss: 1.4865, Validation Accuracy: 42.97%\n",
      "Epoch 3/50, Training Loss: 0.3631, Training Accuracy: 41.37%, Validation Loss: 1.4310, Validation Accuracy: 41.11%\n",
      "Epoch 4/50, Training Loss: 0.3411, Training Accuracy: 43.29%, Validation Loss: 1.3738, Validation Accuracy: 40.05%\n",
      "Epoch 5/50, Training Loss: 0.3169, Training Accuracy: 51.38%, Validation Loss: 1.2930, Validation Accuracy: 51.46%\n",
      "Epoch 6/50, Training Loss: 0.3126, Training Accuracy: 50.49%, Validation Loss: 1.3534, Validation Accuracy: 50.40%\n",
      "Epoch 7/50, Training Loss: 0.2970, Training Accuracy: 54.04%, Validation Loss: 1.2649, Validation Accuracy: 53.58%\n",
      "Epoch 8/50, Training Loss: 0.2807, Training Accuracy: 55.91%, Validation Loss: 1.2851, Validation Accuracy: 52.52%\n",
      "Epoch 9/50, Training Loss: 0.2736, Training Accuracy: 55.63%, Validation Loss: 1.3686, Validation Accuracy: 55.17%\n",
      "Epoch 10/50, Training Loss: 0.2548, Training Accuracy: 60.40%, Validation Loss: 1.2708, Validation Accuracy: 48.01%\n",
      "Epoch 11/50, Training Loss: 0.2511, Training Accuracy: 61.38%, Validation Loss: 1.2381, Validation Accuracy: 57.29%\n",
      "Epoch 12/50, Training Loss: 0.2321, Training Accuracy: 62.97%, Validation Loss: 1.2597, Validation Accuracy: 53.85%\n",
      "Epoch 13/50, Training Loss: 0.2298, Training Accuracy: 64.66%, Validation Loss: 1.2669, Validation Accuracy: 55.17%\n",
      "Epoch 14/50, Training Loss: 0.2130, Training Accuracy: 65.68%, Validation Loss: 1.2809, Validation Accuracy: 62.33%\n",
      "Epoch 15/50, Training Loss: 0.2019, Training Accuracy: 68.82%, Validation Loss: 1.2584, Validation Accuracy: 57.29%\n",
      "Epoch 16/50, Training Loss: 0.1758, Training Accuracy: 71.34%, Validation Loss: 1.2821, Validation Accuracy: 58.36%\n",
      "Early stopping triggered\n",
      "Training Complete\n",
      "Training ResNet50...\n",
      "Epoch 1/50, Training Loss: 0.3673, Training Accuracy: 37.82%, Validation Loss: 1.3080, Validation Accuracy: 46.15%\n",
      "Epoch 2/50, Training Loss: 0.3131, Training Accuracy: 48.76%, Validation Loss: 1.2345, Validation Accuracy: 58.89%\n",
      "Epoch 3/50, Training Loss: 0.2845, Training Accuracy: 55.77%, Validation Loss: 1.2258, Validation Accuracy: 61.01%\n",
      "Epoch 4/50, Training Loss: 0.2620, Training Accuracy: 59.33%, Validation Loss: 1.3363, Validation Accuracy: 51.72%\n",
      "Epoch 5/50, Training Loss: 0.2501, Training Accuracy: 60.82%, Validation Loss: 1.2590, Validation Accuracy: 55.97%\n",
      "Epoch 6/50, Training Loss: 0.2215, Training Accuracy: 64.84%, Validation Loss: 1.3249, Validation Accuracy: 55.97%\n",
      "Epoch 7/50, Training Loss: 0.2006, Training Accuracy: 68.86%, Validation Loss: 1.3872, Validation Accuracy: 57.82%\n",
      "Epoch 8/50, Training Loss: 0.1703, Training Accuracy: 73.82%, Validation Loss: 1.1890, Validation Accuracy: 58.62%\n",
      "Epoch 9/50, Training Loss: 0.1547, Training Accuracy: 75.18%, Validation Loss: 1.1662, Validation Accuracy: 57.29%\n",
      "Epoch 10/50, Training Loss: 0.1593, Training Accuracy: 75.18%, Validation Loss: 1.1705, Validation Accuracy: 60.74%\n",
      "Epoch 11/50, Training Loss: 0.1435, Training Accuracy: 78.12%, Validation Loss: 1.2642, Validation Accuracy: 59.68%\n",
      "Epoch 12/50, Training Loss: 0.1302, Training Accuracy: 80.50%, Validation Loss: 1.3181, Validation Accuracy: 55.44%\n",
      "Epoch 13/50, Training Loss: 0.1355, Training Accuracy: 79.24%, Validation Loss: 1.2734, Validation Accuracy: 59.15%\n",
      "Epoch 14/50, Training Loss: 0.1334, Training Accuracy: 79.48%, Validation Loss: 1.3292, Validation Accuracy: 59.15%\n",
      "Early stopping triggered\n",
      "Training Complete\n",
      "Training EfficientNet...\n",
      "Epoch 1/50, Training Loss: 0.3978, Training Accuracy: 28.61%, Validation Loss: 1.5090, Validation Accuracy: 40.85%\n",
      "Epoch 2/50, Training Loss: 0.3685, Training Accuracy: 40.16%, Validation Loss: 1.3693, Validation Accuracy: 44.83%\n",
      "Epoch 3/50, Training Loss: 0.3418, Training Accuracy: 46.14%, Validation Loss: 1.3039, Validation Accuracy: 50.13%\n",
      "Epoch 4/50, Training Loss: 0.3191, Training Accuracy: 51.71%, Validation Loss: 1.2134, Validation Accuracy: 55.97%\n",
      "Epoch 5/50, Training Loss: 0.3074, Training Accuracy: 53.16%, Validation Loss: 1.2344, Validation Accuracy: 54.91%\n",
      "Epoch 6/50, Training Loss: 0.2907, Training Accuracy: 56.24%, Validation Loss: 1.1902, Validation Accuracy: 59.15%\n",
      "Epoch 7/50, Training Loss: 0.2759, Training Accuracy: 59.00%, Validation Loss: 1.1877, Validation Accuracy: 56.50%\n",
      "Epoch 8/50, Training Loss: 0.2625, Training Accuracy: 60.96%, Validation Loss: 1.1537, Validation Accuracy: 58.89%\n",
      "Epoch 9/50, Training Loss: 0.2507, Training Accuracy: 62.88%, Validation Loss: 1.2205, Validation Accuracy: 58.36%\n",
      "Epoch 10/50, Training Loss: 0.2407, Training Accuracy: 64.14%, Validation Loss: 1.1719, Validation Accuracy: 59.42%\n",
      "Epoch 11/50, Training Loss: 0.2241, Training Accuracy: 66.25%, Validation Loss: 1.2124, Validation Accuracy: 58.09%\n",
      "Epoch 12/50, Training Loss: 0.2080, Training Accuracy: 69.19%, Validation Loss: 1.1763, Validation Accuracy: 60.21%\n",
      "Epoch 13/50, Training Loss: 0.1940, Training Accuracy: 70.92%, Validation Loss: 1.1547, Validation Accuracy: 57.56%\n",
      "Early stopping triggered\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data Augmentation and Normalization for Training and Testing\n",
    "train_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(45),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomErasing(p=0.3),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Custom Dataset for Unlabeled Test Data\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = [f for f in sorted(os.listdir(image_dir)) if f.endswith('.jpg') or f.endswith('.png')]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, self.image_files[idx]\n",
    "\n",
    "# Load Data\n",
    "train_data = datasets.ImageFolder('C:\\MLCPS\\WP\\Project 1 Data\\Project 1 Data\\Train_Data', transform=train_transforms)\n",
    "\n",
    "# Compute class weights to handle imbalance\n",
    "targets = [label for _, label in train_data.samples]\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(targets), y=targets)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "# Create a validation dataset (15% of the data)\n",
    "val_size = int(0.15 * len(train_data))\n",
    "train_size = len(train_data) - val_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_data, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "test_data = TestDataset('C:\\MLCPS\\WP\\Project 1 Data\\Project 1 Data\\Test_Data', transform=test_transforms)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "\n",
    "# Define the Models (VGG16, ResNet50, EfficientNet)\n",
    "vgg16_model = models.vgg16(pretrained=True)\n",
    "for param in vgg16_model.features[-10:]:\n",
    "    param.requires_grad = True\n",
    "vgg16_model.classifier[6] = nn.Linear(vgg16_model.classifier[6].in_features, 5)  # Adjust for 5 classes\n",
    "vgg16_model = vgg16_model.to(device)\n",
    "\n",
    "resnet50_model = models.resnet50(pretrained=True)\n",
    "for param in resnet50_model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "resnet50_model.fc = nn.Linear(resnet50_model.fc.in_features, 5)  # Adjust for 5 classes\n",
    "resnet50_model = resnet50_model.to(device)\n",
    "\n",
    "efficientnet_model = models.efficientnet_b0(pretrained=True)\n",
    "for param in efficientnet_model.features[-2:]:\n",
    "    param.requires_grad = True\n",
    "efficientnet_model.classifier[1] = nn.Linear(efficientnet_model.classifier[1].in_features, 5)  # Adjust for 5 classes\n",
    "efficientnet_model = efficientnet_model.to(device)\n",
    "\n",
    "# Function to ensemble predictions by averaging softmax scores with weights\n",
    "def ensemble_predict_weighted(models, loader, weights):\n",
    "    model_predictions = []\n",
    "    for i, model in enumerate(models):\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for inputs, _ in loader:\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                probs = F.softmax(outputs, dim=1)  # Get softmax probabilities\n",
    "                predictions.append(probs.cpu().numpy() * weights[i])  # Apply weight to predictions\n",
    "        model_predictions.append(np.vstack(predictions))\n",
    "    \n",
    "    avg_predictions = np.sum(model_predictions, axis=0)\n",
    "    return np.argmax(avg_predictions, axis=1)  # Return the class with the highest weighted average probability\n",
    "\n",
    "# Gradient Accumulation and Mixed Precision Training\n",
    "scaler = GradScaler()  # For mixed precision training\n",
    "accumulation_steps = 4  # Accumulate gradients over 4 steps\n",
    "\n",
    "# Early stopping parameters\n",
    "early_stopping_patience = 5\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Train each model\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50):\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)  # Use class weights in loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "\n",
    "    global epochs_no_improve, best_val_loss\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for i, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            with autocast():  # Mixed precision\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss = loss / accumulation_steps  # Gradient accumulation\n",
    "                scaler.scale(loss).backward()\n",
    "\n",
    "            if (i + 1) % accumulation_steps == 0:  # Update weights after gradient accumulation\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Track training accuracy\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct_train += torch.sum(preds == labels).item()\n",
    "            total_train += labels.size(0)\n",
    "        \n",
    "        # Calculate training loss and accuracy\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_acc = 100 * correct_train / total_train\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                correct_val += torch.sum(preds == labels).item()\n",
    "                total_val += labels.size(0)\n",
    "\n",
    "        # Calculate validation accuracy and loss\n",
    "        val_acc = 100 * correct_val / total_val\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "\n",
    "        scheduler.step(val_loss)  # Step learning rate scheduler\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
    "              f'Training Loss: {epoch_loss:.4f}, Training Accuracy: {train_acc:.2f}%, '\n",
    "              f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%')\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve == early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    print('Training Complete')\n",
    "\n",
    "# Train the models individually\n",
    "print(\"Training VGG16...\")\n",
    "train_model(vgg16_model, train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "print(\"Training ResNet50...\")\n",
    "train_model(resnet50_model, train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "print(\"Training EfficientNet...\")\n",
    "train_model(efficientnet_model, train_loader, val_loader, num_epochs=50)\n",
    "\n",
    "# Ensemble the models with weighted averaging\n",
    "models = [vgg16_model, resnet50_model, efficientnet_model]\n",
    "weights = [0.4, 0.3, 0.3]  # Adjust weights based on individual model performance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file saved as 'Trial_7\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions on the test set\n",
    "test_preds = ensemble_predict_weighted(models, test_loader, weights)\n",
    "\n",
    "# Prepare submission\n",
    "test_image_ids = [os.path.splitext(fname)[0] for _, fname in test_data]\n",
    "sample_submission = pd.DataFrame({'ID': test_image_ids, 'Predictions': test_preds + 1})  # Adjust class numbering if needed\n",
    "sample_submission = sample_submission.sort_values(by='ID')\n",
    "sample_submission.to_csv('F_1.csv', index=False)\n",
    "print(\"Submission file saved as 'Trial_7\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
